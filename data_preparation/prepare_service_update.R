## PART 0 - SCRIPT INFORMATION SECTION ----------------------------------------

#' The script can be used to preprocess raw field data. It can be used, with 
#' the repsective toggle options, to handle Cube and Centaur data loggers, and 
#' also to work with break-out boxes, separating one 3-cmp channel to three 
#' 1-cmp channels.
#'
#' It shall be used to coherenty handle all station maintenance and data 
#' collection services. This includes converting files from the different 
#' loggers to sac files and organise them 
#' in the standard directory structure used in the Geomorphology Section  
#' for later analysis. The result is copied to the SecGM STORAGE drive
#'
#' For operation, carefully check the settings section (PART 1) for appropriate 
#' argument values. This concerns especially the service number and if there 
#' were any changes of either the data archive structure, the seismic data 
#' processing machine, or physical changes of the seismic network.
#'
#' The script must be run line by line. It is not intended to be run all 
#' togeter. Between semi-automated sections there is the need to manually 
#' perform jobs using the Linux console of the remote Linux server, by
#' copy paste commands generated by this script.
#' 
#' Author: Michael Dietze, Uni Goettingen (michael.dietze)
#' 
#' Script version: 0.5.0
#' 
#' Changes to previous versions
#'   - adaption to new virtual server and path definitions
#'   - complete rework of the code
#'   - support of different data loggers and storage approaches
#'   - split channels
#' 
#' Dependencies: 
#'   - R 4.n
#'   - eseis 0.7.1
#'   
#'   Parameter descriptions:
#'   - opt, general options
#'     - service, service ID which must meet the service directory name where 
#'       the service's raw cube or centaur data are stored. Only directory 
#'       name is to be used, the path will be added, see below.
#'     - ID_exclude, station IDs to exclude from service update. Useful if a 
#'       service did not include all stations of a network. All not excluded
#'       stations will be tried to process, which will cause errors of those
#'       do not exist in the service directory.
#'     - station_split, option to further process converted files in case a 
#'       break-out box or splitter box was used. Some stations have this, 
#'       menaing that a 3-component signal is split into three 1-component 
#'       signals, hence data coming from three 1-component sensors. In the case
#'       that station_split is set TRUE, the routine will look for a station 
#'       info file column that defines which channel to asign to which 
#'       individual 1-component sensor, the column must be named "chn_cube",
#'       see station documentation part below.
#'     - cube_fringe, option for gipptools software to also convert seismic 
#'       data beyond the daily file's first and last time stamp. To get a 
#'       continuous stream of data it is recommended to set this to TRUE.
#'     - cpu, fraction of CPUs to use during conversion.
#'     - remove_temp, option to delete temporary files after conversion.
#'     - packages, vector of packages that will be loaded in order to process
#'       the data.
#'  - path, path information
#'     - storage, path to directory where the station service directory is 
#'       present. The script assumes that there is a place where all collected 
#'       field data is stored, and which is organsied by field maintenance or
#'       service visits. In each service directory, there must be directories
#'       named by the Cube logger IDs and that contain all collected raw Cube
#'       files without any furthe sub-directories. If a Cube logger reboots 
#'       during data collection, it will create a new directory. Thus before 
#'       attempting the conversion, all sub-directory structures must be 
#'       resolved.
#'     - temp, path to an existing temporary directory. The routine will 
#'       create several files and directories in that one. Make sure that the 
#'       directory is writable and has sufficient space to keep all the data.
#'     - gipptools, path to the GIPPtools directory that contains routines to 
#'       convert Cube files to mseed files. 
#'     - station, path and name of the station info file. A station info file 
#'       ideally contains the following information: "ID"	"name"	"x"	"y"	"z"	
#'       "d"	"sensor_type"	"logger_type"	"sensor_ID"	"chn_cube"	"logger_ID"
#'       "gain"	"date_start"	"date_stop"	"service", organised as columns and 
#'       each lines containing an entry of a station and service. The routine 
#'       will isolate the stations to process based on the correctly specified 
#'       service number and exclude the stations to be excluded (see above).

## PART 1 - SETTINGS SECTION --------------------------------------------------

pars <- list(
  
  opt = list(service = "service_nn",
             ID_exclude = c(),
             cube_verbose = TRUE,
             station_split = FALSE,
             cube_fringe = "constant",
             cpu = 1 / 11,
             remove_temp = TRUE,
             packages = "eseis"),
  
  path = list(storage = paste0("/data/STORAGE/Seismic/Europe/Germany/nn/",
                               "data/seismic/"),
              temp = paste0("/data/DATA/usernam/00_TEMP/"),
              gipptools = paste0("/software/gipptools-2022.171/"),
              station = paste0("/data/STORAGE/Seismic/Europe/Germany/",
                               "nn/data/seismic/station/",
                               "station_info_nn.txt")))

## PART 2 - PREPARATION OF DATA -----------------------------------------------

## prepare environment
x <- lapply(pars$opt$packages, require, character.only = TRUE)
rm(x)
Sys.setenv(TZ = "UTC")

## optionally process cube files ----------------------------------------------

## create temporary directories
key <- format(Sys.time(), format = "%Y%m%d%H%M%S")
temp_station <- paste0(pars$path$temp, "temp_station_", key)
temp_sac <- paste0(pars$path$temp, "temp_sac_", key)

if(dir.exists(temp_station) == FALSE) {
  
  dir.create(temp_station)
}

if(dir.exists(temp_sac) == FALSE) {
  
  dir.create(temp_sac)
}

## isolate stations with cubes as data loggers
stations <- read.table(file = pars$path$station,
                       header = TRUE,
                       stringsAsFactors = FALSE)

stations <- stations[!is.na(stations$logger_type),]

stations_cube <- stations[stations$logger_type == "Cube3ext" | 
                            stations$logger_type == "Cube3extBOB",]

## remove stations to exclude
stations_cube <- stations_cube[is.na(match(x = stations_cube$ID, 
                                           table = pars$opt$ID_exclude)),]

## isolate service of interest
stations_cube <- stations_cube[stations_cube$service == as.numeric(
  gsub(x = pars$opt$service, pattern = "service_", replacement = "")),]

## write updated station info file to temporary directory
write.table(x = stations_cube, 
            file = paste0(temp_station, 
                          "/stations_cube.txt"))

## organise cube files
aux_organisecubefiles(stationfile = paste0(temp_station, 
                                           "/stations_cube.txt"),
                      input_dir = paste0(pars$path$storage, 
                                         "cube/", 
                                         pars$opt$service),
                      output_dir = temp_sac,
                      gipptools = pars$path$gipptools,
                      heapspace = 4 * 4096,
                      verbose = pars$opt$cube_verbose,
                      fringe = pars$opt$cube_fringe, 
                      cpu = pars$opt$cpu)

## remove temporary files and directories
unlink(x = temp_station,
       recursive = TRUE)

## optionally process mseed files ---------------------------------------------
stations_centaur <- stations[stations$logger_type == "Centaur",]

## remove stations to exclude
if(nrow(stations_centaur) > 0) {
  stations_centaur <- 
    stations_centaur[is.na(match(x = stations_centaur$ID, 
                                 table = pars$opt$ID_exclude)),]
}

## isolate serice of interest
stations_centaur <- stations_centaur[stations_centaur$service == as.numeric(
  gsub(x = pars$opt$service, pattern = "service_", replacement = "")),]

if(nrow(stations_centaur) > 0) {
  
  ## make list of mseed file to process
  mseed <- c(list.files(path = paste(pars$path$storage,
                                     "centaur/",
                                     pars$opt$service,
                                     sep = ""),
                        pattern = "miniseed",
                        recursive = TRUE,
                        full.names = TRUE),
             list.files(path = paste(pars$path$storage,
                                     "centaur/",
                                     pars$opt$service,
                                     sep = ""),
                        pattern = "mseed",
                        recursive = TRUE,
                        full.names = TRUE))
  
  ## isolate files of interest
  for(i in 1:nrow(stations_centaur)) {
    
    mseed <- mseed[grepl(x = mseed, pattern = stations_centaur$logger_ID[i])]
  }
  
  for(i in 1:length(mseed)) {
    
    ## read mseed file
    x <- try(eseis::read_mseed(file = mseed[i]))
    
    ## check if import was successful
    if(class(x)[1] == "eseis") {
      
      ## make hourly time sequence
      t_cut <- seq(from = as.POSIXct(format(x$meta$starttime, 
                                            "%Y-%m-%d %H:%M:%S")), 
                   to = x$meta$starttime + x$meta$dt * x$meta$n - x$meta$dt,
                   by = 3600)
      
      ## loop through all time snippets
      for(j in 1:length(t_cut)) {
        
        ## clip data set
        x_clip <- try(signal_clip(data = x, limits = c(t_cut[j],
                                                       t_cut[j] + 3600)))
        
        ## get station ID
        ID <- strsplit(x = x_clip$meta$filename, 
                       split = "/", 
                       fixed = TRUE)[[1]]
        ID <- ID[length(ID)]
        
        ## match station ID and update meta info
        ID_i <- do.call(c, lapply(X = paste0("_", 
                                             stations_centaur$logger_ID, 
                                             "_"), FUN = function(id, ID) {
                                               
                                               grepl(pattern = id, x = ID)
                                             }, ID))
        x_clip$meta$station <- stations_centaur$ID[ID_i]
        
        ## update component info
        if(grepl(x = x_clip$meta$component, pattern = "X")) {
          x_clip$meta$component <- "BHE"
        }
        if(grepl(x = x_clip$meta$component, pattern = "Y")) {
          x_clip$meta$component <- "BHN"
        }
        if(grepl(x = x_clip$meta$component, pattern = "Z")) {
          x_clip$meta$component <- "BHZ"
        }
        
        ## make directory structure
        y <- format(x = x_clip$meta$starttime, "%Y")
        yjd <- format(x = x_clip$meta$starttime, "%Y/%j")
        
        if(dir.exists(paste0(temp_sac, "/", y)) == FALSE) {
          
          dir.create(paste0(temp_sac, "/", y))
        }

        if(dir.exists(paste0(temp_sac, "/", yjd)) == FALSE) {
          
          dir.create(paste0(temp_sac, "/", yjd))
        }
        
        ## create sac file name
        file_sac_save <- paste0(temp_sac, "/",
                                yjd, "/",
                                x_clip$meta$station, ".",
                                format(x_clip$meta$starttime, "%y."),
                                format(x_clip$meta$starttime, "%j."),
                                format(x_clip$meta$starttime, "%H."),
                                format(x_clip$meta$starttime, "%M."),
                                format(x_clip$meta$starttime, "%S."),
                                x_clip$meta$component, ".SAC")
        
        ## write sac file
        write_sac(data = x_clip, file = file_sac_save, unit = "unknown")
      }
    }
  }
}

## optionally prepare split channel files -------------------------------------
# if(pars$opt$station_split == TRUE) {
# 
#   ## make list of sac files
#   files_sac_in <- list.files(path = temp_sac, 
#                              recursive = TRUE, 
#                              full.names = TRUE)
#   
#   
#   ## process sac files
#   for(i in 1:length(files_sac_in)) {
#     
#     ## print progress
#     print(i)
#     
#     ## load file
#     s <- read_sac(file = files_sac_in[i])
#     
#     ## get meta data
#     cmp <- s$meta$component
#     sta <- s$meta$station
#     
#     ## get corresponding new meta data
#     ID_out <- stations_split$ID[which(stations_split$ID_raw == sta & 
#                                         stations_split$chn_cube == cmp)]
#     
#     meta_split <- stations_split[stations_split$ID == ID_out,]
#     
#     ## update meta data
#     s$meta$station <- meta_split$ID
#     s$meta$component <- meta_split$ID <- "BHZ"
#     
#     ## create new file name
#     s_name <- paste0(temp_sac, "/", 
#                      format(s$meta$starttime, "%Y/%j"), "/",
#                      s$meta$station, ".",
#                      format(s$meta$starttime, "%y.%j.%H.%M.%S."),
#                      s$meta$component, ".SAC")
#     
#     ## write sac file
#     write_sac(data = s, file = s_name, unit = "unknown")
#   }
#   
#   ## delete original sac files
#   for(i in 1:length(files_sac_in)) {
#     
#     unlink(x = files_sac_in[i], recursive = TRUE)
#   }
# }
